{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "canadian-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import csv\n",
    "import copy\n",
    "\n",
    "from math import log\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "from hanMaskingPackage import weight_perc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eb9ee60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# device = \"cpu\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2252e19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa828175d30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_folds = 5\n",
    "num_of_epoch = 100\n",
    "output_size = 1\n",
    "binary_classification = True\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1d077f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_Layer_1 = 256\n",
    "hidden_Layer_2 = 128\n",
    "hidden_Layer_3 = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddea7075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(train_df, test_df, y_column_name):\n",
    "    y_train = train_df[y_column_name]\n",
    "    X_train = train_df.drop([y_column_name], axis=1)\n",
    "\n",
    "    y_test = test_df[y_column_name]\n",
    "    X_test = test_df.drop([y_column_name], axis=1)\n",
    "\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    scale = min_max_scaler.fit(X_train)\n",
    "    \n",
    "    x_train_transformed = scale.transform(X_train)\n",
    "    x_test_transformed = scale.transform(X_test)\n",
    "\n",
    "    train_df_standardized = pd.DataFrame(x_train_transformed, columns = X_train.columns)\n",
    "    test_df_standardized = pd.DataFrame(x_test_transformed, columns = X_test.columns)\n",
    "\n",
    "    train_df = pd.concat([train_df_standardized.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
    "    test_df = pd.concat([test_df_standardized.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07b17843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_dataset(train_df, test_df, y_column_name):\n",
    "\n",
    "    y_train = train_df[y_column_name]\n",
    "    X_train = train_df.drop([y_column_name], axis=1)\n",
    "\n",
    "    y_test = test_df[y_column_name]\n",
    "    X_test = test_df.drop([y_column_name], axis=1)\n",
    "\n",
    "    # standardize\n",
    "    sc = StandardScaler()\n",
    "    scale = sc.fit(X_train)\n",
    "    \n",
    "    x_train_transformed = scale.transform(X_train)\n",
    "    x_test_transformed = scale.transform(X_test)\n",
    "\n",
    "    train_df_standardized = pd.DataFrame(x_train_transformed, columns = X_train.columns)\n",
    "    \n",
    "    test_df_standardized = pd.DataFrame(x_test_transformed, columns = X_test.columns)\n",
    "\n",
    "    train_df = pd.concat([train_df_standardized.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
    "    test_df = pd.concat([test_df_standardized.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cf93e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read files\n",
    "def file_reader(file_path):\n",
    "    '''Input = file path (str)\n",
    "       Output = numpy array of items in files\n",
    "    '''\n",
    "    \n",
    "    data = []\n",
    "    with open(file_path) as f:\n",
    "        reader = csv.reader(f, delimiter='\\n')\n",
    "        for row in reader:\n",
    "            for x in row:\n",
    "                x=x.split(' ')\n",
    "                example = []\n",
    "                for item in x:\n",
    "                    if item:\n",
    "                        item = int(item) #convert to int\n",
    "                        example.append(item)\n",
    "                data.append(example)\n",
    "        data = np.asarray(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3385bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## arcene\n",
    "dbName = 'arcene'\n",
    "\n",
    "arcene_train_X = file_reader('../hd-datasets/ARCENE/arcene_train.data')\n",
    "arcene_test_X = file_reader('../hd-datasets/ARCENE/arcene_valid.data')\n",
    "\n",
    "arcene_train_y = file_reader('../hd-datasets/ARCENE/arcene_train.labels')\n",
    "arcene_train_y = np.ravel(arcene_train_y)\n",
    "arcene_test_y = file_reader('../hd-datasets/ARCENE/arcene_valid.labels')\n",
    "arcene_test_y = np.ravel(arcene_test_y)\n",
    "\n",
    "arcene_train = np.column_stack( (arcene_train_X,arcene_train_y) )\n",
    "arcene_test = np.column_stack( (arcene_test_X,arcene_test_y) )\n",
    "arcene = np.row_stack( (arcene_train,arcene_test) )\n",
    "\n",
    "data_df = pd.DataFrame.from_records(arcene)\n",
    "y_column_name = 10000\n",
    "\n",
    "le = LabelEncoder()\n",
    "data_df[y_column_name] = le.fit_transform(data_df[y_column_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e33dbab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9991</th>\n",
       "      <th>9992</th>\n",
       "      <th>9993</th>\n",
       "      <th>9994</th>\n",
       "      <th>9995</th>\n",
       "      <th>9996</th>\n",
       "      <th>9997</th>\n",
       "      <th>9998</th>\n",
       "      <th>9999</th>\n",
       "      <th>10000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>538</td>\n",
       "      <td>404</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>570</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>524</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>82</td>\n",
       "      <td>165</td>\n",
       "      <td>60</td>\n",
       "      <td>554</td>\n",
       "      <td>379</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>605</td>\n",
       "      <td>69</td>\n",
       "      <td>7</td>\n",
       "      <td>473</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>423</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>451</td>\n",
       "      <td>402</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>593</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>508</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>44</td>\n",
       "      <td>275</td>\n",
       "      <td>14</td>\n",
       "      <td>511</td>\n",
       "      <td>470</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>600</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>469</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>348</td>\n",
       "      <td>0</td>\n",
       "      <td>268</td>\n",
       "      <td>329</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>301</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>24</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>436</td>\n",
       "      <td>92</td>\n",
       "      <td>400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>139</td>\n",
       "      <td>261</td>\n",
       "      <td>...</td>\n",
       "      <td>540</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>130</td>\n",
       "      <td>365</td>\n",
       "      <td>58</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>11</td>\n",
       "      <td>58</td>\n",
       "      <td>50</td>\n",
       "      <td>332</td>\n",
       "      <td>109</td>\n",
       "      <td>393</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>134</td>\n",
       "      <td>...</td>\n",
       "      <td>355</td>\n",
       "      <td>156</td>\n",
       "      <td>77</td>\n",
       "      <td>26</td>\n",
       "      <td>277</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>261</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>93</td>\n",
       "      <td>32</td>\n",
       "      <td>137</td>\n",
       "      <td>319</td>\n",
       "      <td>0</td>\n",
       "      <td>264</td>\n",
       "      <td>231</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>244</td>\n",
       "      <td>309</td>\n",
       "      <td>0</td>\n",
       "      <td>276</td>\n",
       "      <td>312</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>119</td>\n",
       "      <td>12</td>\n",
       "      <td>198</td>\n",
       "      <td>339</td>\n",
       "      <td>0</td>\n",
       "      <td>289</td>\n",
       "      <td>410</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>256</td>\n",
       "      <td>402</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>350</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>112</td>\n",
       "      <td>19</td>\n",
       "      <td>171</td>\n",
       "      <td>334</td>\n",
       "      <td>0</td>\n",
       "      <td>282</td>\n",
       "      <td>208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>226</td>\n",
       "      <td>379</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>367</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 10001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0      1      2      3      4      5      6      7      8      9      \\\n",
       "0        0     71      0     95      0    538    404     20      0      0   \n",
       "1        0     41     82    165     60    554    379      0     71      0   \n",
       "2        0      0      1     40      0    451    402      0      0      0   \n",
       "3        0     56     44    275     14    511    470      0      0      0   \n",
       "4      105      0    141    348      0    268    329      0      0      1   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "195     24     73      0    436     92    400      0      0    139    261   \n",
       "196     11     58     50    332    109    393    122      0     75    134   \n",
       "197     93     32    137    319      0    264    231     21      0      0   \n",
       "198    119     12    198    339      0    289    410      0      0      4   \n",
       "199    112     19    171    334      0    282    208      0      0      0   \n",
       "\n",
       "     ...  9991   9992   9993   9994   9995   9996   9997   9998   9999   10000  \n",
       "0    ...    570     86      0     36      0     80      0      0    524      1  \n",
       "1    ...    605     69      7    473      0     57      0    284    423      0  \n",
       "2    ...    593     28      0     24      0     90      0     34    508      1  \n",
       "3    ...    600      0     26     86      0    102      0      0    469      1  \n",
       "4    ...      0      0      0      0    190    301      0      0    354      0  \n",
       "..   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "195  ...    540      0     86    130    365     58     17      3     37      0  \n",
       "196  ...    355    156     77     26    277    265      0     36    261      0  \n",
       "197  ...      9      0      0      0    244    309      0    276    312      1  \n",
       "198  ...      0     37      0      0    256    402      0      0    350      1  \n",
       "199  ...      0    118      0      0    226    379      0      0    367      0  \n",
       "\n",
       "[200 rows x 10001 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe6806ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_multiple_lists(multiple_lists):\n",
    "    data = np.array(multiple_lists)\n",
    "    return np.average(data, axis=0), np.std(data,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80e1cf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic auto encoder with three layers\n",
    "class Basic_DNN_3(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size = 1):\n",
    "\n",
    "        hidden_L_1 = hidden_Layer_1\n",
    "        hidden_L_2 = hidden_Layer_2\n",
    "        hidden_L_3 = hidden_Layer_3\n",
    "        \n",
    "\n",
    "        super().__init__()        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_L_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_L_1, hidden_L_2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.20),\n",
    "            nn.Linear(hidden_L_2, hidden_L_3),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.20),\n",
    "            nn.Linear(hidden_L_3, output_size),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.20)\n",
    "        )\n",
    "\n",
    "        if binary_classification:\n",
    "            self.activation = nn.Sigmoid()\n",
    "\n",
    "    # fine-tuning\n",
    "    def forward(self, x):\n",
    "        intermediate = self.encoder(x)\n",
    "        if binary_classification:\n",
    "            return self.activation(intermediate)\n",
    "        else:\n",
    "            # no activation function is used for multiclass classification\n",
    "            # cross entrophy loss automatically applies softmax in pytorch\n",
    "            return intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca25f228-e96a-4c4e-89c2-51d9f04703d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compressions = []\n",
    "\n",
    "def calculate_model_compression(model):\n",
    "    perc_weight = []\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    for name, param in state_dict.items():\n",
    "\n",
    "        # ignore biases and the decoder weights\n",
    "        if not \"weight\" in name or \"decoder\" in name:\n",
    "            continue\n",
    "\n",
    "        W =  param.cpu().numpy()\n",
    "        cnt_zero = len(np.ravel(W))-np.count_nonzero(W)\n",
    "        perc = (( cnt_zero)*100)/len(np.ravel(W))\n",
    "        perc_weight.append(perc)\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model_compressions.append(sum(perc_weight)/len(perc_weight))\n",
    "    return model\n",
    "    # print('Achieved model compression: ',sum(perc_weight)/len(perc_weight), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edfa74ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_perturbation(model, epoch, perc_weight, threshold, Wold_dict):\n",
    "\n",
    "    state_dict = model.state_dict()\n",
    "    \n",
    "\n",
    "    for name, param in state_dict.items():\n",
    "\n",
    "        # ignore biases and the decoder weights\n",
    "        if not \"weight\" in name or \"decoder\" in name:\n",
    "            continue\n",
    "\n",
    "\n",
    "        W =  param.cpu().numpy()\n",
    "        if name not in Wold_dict:\n",
    "            Wold = np.ones(W.shape)\n",
    "        else:\n",
    "            Wold = Wold_dict[name]\n",
    "\n",
    "        Wold, perc = weight_perc(Wold,W,threshold)\n",
    "        Wold_dict[name] = Wold\n",
    "        perc_weight.append(perc)\n",
    "        wm = np.multiply(Wold,W)\n",
    "        wm = torch.from_numpy(wm)\n",
    "        state_dict[name].copy_(wm)\n",
    "\n",
    "        # print('The average of weights perturbation: ',sum(perc_weight)/len(perc_weight), '%')\n",
    "\n",
    "    return perc_weight, Wold_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8f0a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_finetuning(model, train_tensor, num_of_epoch, train_x, train_y, test_x, test_y, threshold):\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    finetuning_loss = []\n",
    "    train_accuracy_scores = []\n",
    "    test_accuracy_scores = []\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=train_tensor,\n",
    "                                        batch_size=64,\n",
    "                                        shuffle=True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if binary_classification:\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    minimum_loss = None\n",
    "\n",
    "    limit_counter = 0\n",
    "    for epoch in range(num_of_epoch):\n",
    "        model = model.train()\n",
    "        for train,target in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # to make sure all variables are float type\n",
    "            train = train.float()\n",
    "            target = target.float()\n",
    "\n",
    "            prediction = model(train)\n",
    "            # target = target.unsqueeze(1)\n",
    "            classification_loss = criterion(prediction, target.unsqueeze(1))\n",
    "            optimizer.zero_grad()\n",
    "            classification_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            \n",
    "        finetuning_loss.append(classification_loss.item())\n",
    "\n",
    "        # train accuracy after each epoch\n",
    "        model = model.eval()\n",
    "        prediction_train = model(train_x)\n",
    "        prediction_train = prediction_train.flatten()\n",
    "        train_accuracy = accuracy_score(train_y.cpu().detach().numpy(),np.round(prediction_train.cpu().detach().numpy()))\n",
    "        train_accuracy_scores.append(train_accuracy)\n",
    "\n",
    "        # test accuracy after each epoch\n",
    "        prediction_test = model(test_x)\n",
    "        prediction_test = prediction_test.flatten()\n",
    "        test_accuracy = accuracy_score(test_y.cpu().detach().numpy(),np.round(prediction_test.cpu().detach().numpy()))\n",
    "        test_accuracy_scores.append(test_accuracy)\n",
    "        \n",
    "    # perturbation variables\n",
    "    perc_weight = []\n",
    "    Wold_dict = {}\n",
    "    perc_weight, Wold_dict = weight_perturbation(model, epoch, perc_weight, threshold, Wold_dict)\n",
    "\n",
    "\n",
    "    # load and return the model with minimum loss at the end\n",
    "    model = calculate_model_compression(model)\n",
    "    \n",
    "    return model, finetuning_loss, train_accuracy_scores ,test_accuracy_scores, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31bb74cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_tensor(df):\n",
    "    y_train = df[y_column_name]\n",
    "    X_train = df.drop([y_column_name], axis=1)\n",
    "    y_tensor = torch.tensor(y_train.values)\n",
    "    x_tensor = torch.tensor(X_train.values)\n",
    "    x_tensor = x_tensor.to(device)\n",
    "    y_tensor = y_tensor.to(device)\n",
    "\n",
    "    # commented out normalization here because data is standardized\n",
    "    # x_tensor_norm = torch.nn.functional.normalize(x_tensor, p=2.0, dim=1, eps=1e-12, out=None)\n",
    "    \n",
    "    dataset_tensor = data_utils.TensorDataset(x_tensor, y_tensor)\n",
    "    return x_tensor, y_tensor, dataset_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d2e5f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_DNN_training(model_name, dataset, num_of_epoch, k_folds, threshold):\n",
    "\n",
    "    kfold = KFold(n_splits = k_folds, random_state= 42, shuffle = True)\n",
    "\n",
    "    finetuning_loss_fold = []\n",
    "    train_accuracy_fold = []\n",
    "    test_accuracy_fold = []\n",
    "    classification_accuracy_fold = []\n",
    "    f1_fold = []\n",
    "    \n",
    "    fold_counter = 0\n",
    "    # run k fold in loop\n",
    "    for train, test in list(kfold.split(dataset)):\n",
    "\n",
    "        fold_counter+=1\n",
    "\n",
    "        # divide the data for train and test\n",
    "        train_df = dataset.iloc[train]\n",
    "        test_df =  dataset.iloc[test]\n",
    "\n",
    "        train_df, test_df = standardize_dataset(train_df, test_df, y_column_name)\n",
    "        # train_df= normalize_dataset(train_df, y_column_name)\n",
    "        # test_df= normalize_dataset(test_df, y_column_name)\n",
    "\n",
    "        # convert dataframe into train and test tensor\n",
    "        train_x, train_y, train_tensor = convert_df_to_tensor(train_df)\n",
    "        test_x, test_y, test_tensor = convert_df_to_tensor(test_df)\n",
    "\n",
    "        x_dim = train_x.shape[1]\n",
    "        \n",
    "        model = Basic_DNN_3(input_size=x_dim)\n",
    "\n",
    "        model, finetuning_loss, train_accuracy, test_accuracy, prediction = model_finetuning(model, train_tensor, num_of_epoch, train_x.float(), train_y.float(), test_x.float(), test_y.float(), threshold)\n",
    "\n",
    "        finetuning_loss_fold.append(finetuning_loss)\n",
    "        train_accuracy_fold.append(train_accuracy)\n",
    "        test_accuracy_fold.append(test_accuracy)\n",
    "\n",
    "        # calculate final test accuracy, confusion matrix etc\n",
    "        test_x = test_x.float()\n",
    "        prediction_tensor = model(test_x)\n",
    "        classification_accuracy_fold.append(accuracy_score(test_y.cpu().detach().numpy(),np.round(prediction_tensor.cpu().detach().numpy().flatten())))\n",
    "        f1_fold.append(f1_score(test_y.cpu().detach().numpy(),np.round(prediction_tensor.cpu().detach().numpy().flatten()), average='weighted'))\n",
    "        report = classification_report(test_y.cpu().detach().numpy(),np.round_(prediction_tensor.cpu().detach().numpy().flatten()), output_dict=True)\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "        report_df.to_csv(dbName+\"_\"+model_name+\"_\"+str(fold_counter), index= True)\n",
    "            \n",
    "        finetuning_loss = average_multiple_lists(finetuning_loss_fold)\n",
    "        train_accuracy_curve = average_multiple_lists(train_accuracy_fold)\n",
    "        test_accuracy_curve = average_multiple_lists(test_accuracy_fold)\n",
    "\n",
    "    classification_accuracy_mean = np.round(statistics.mean(classification_accuracy_fold),4)\n",
    "    classification_accuracy_std = np.round(statistics.pstdev(classification_accuracy_fold),4)\n",
    "    print(\"test accuracy:\",classification_accuracy_mean,\" (\",classification_accuracy_std,\")\")\n",
    "    \n",
    "    f1_mean = np.round(statistics.mean(f1_fold),4)\n",
    "    f1_std = np.round(statistics.pstdev(f1_fold),4)\n",
    "    print(\"test f1:\",f1_mean,\" (\",f1_std,\")\")\n",
    "    \n",
    "    return finetuning_loss, train_accuracy_curve, test_accuracy_curve, classification_accuracy_mean, f1_mean, f1_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32807cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shaded_plot(pair, label, x_label, y_label, filename):\n",
    "    mean = pair[0]\n",
    "    std = pair[1]\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    x = np.arange(len(mean))\n",
    "    plt.plot(x, mean, label=label)\n",
    "    plt.fill_between(x, mean - std, mean + std, alpha=0.2)\n",
    "    plt.legend(prop={'size': 13})\n",
    "    plt.xlabel(x_label,fontsize=13)\n",
    "    plt.ylabel(y_label,fontsize=13)\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def shaded_plot_multiple(pairs, labels, x_label, y_label, filename):\n",
    "    \n",
    "    plt.figure(figsize=(9, 6))\n",
    "\n",
    "    for pair, label in zip(pairs, labels):\n",
    "        mean = pair[0]\n",
    "        std = pair[1]\n",
    "        x = np.arange(len(mean))\n",
    "        plt.plot(x, mean, label=label)\n",
    "        plt.fill_between(x, mean - std, mean + std, alpha=0.2)\n",
    "    \n",
    "    plt.legend(prop={'size': 13})\n",
    "    plt.xlabel(x_label,fontsize=13)\n",
    "    plt.ylabel(y_label,fontsize=13)\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87f52074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnn/arcene/\n",
      "Threshold:  2\n",
      "test accuracy: 0.65  ( 0.0791 )\n",
      "test f1: 0.5856  ( 0.1297 )\n",
      "Basic average compressions:  2.2822343750000003\n",
      "Threshold:  4\n",
      "test accuracy: 0.685  ( 0.0644 )\n",
      "test f1: 0.6766  ( 0.0795 )\n",
      "Basic average compressions:  4.173069091796876\n",
      "Threshold:  6\n",
      "test accuracy: 0.665  ( 0.0682 )\n",
      "test f1: 0.6024  ( 0.1392 )\n",
      "Basic average compressions:  6.064668701171875\n",
      "Threshold:  8\n",
      "test accuracy: 0.565  ( 0.1722 )\n",
      "test f1: 0.5462  ( 0.1854 )\n",
      "Basic average compressions:  8.34613232421875\n",
      "Threshold:  10\n",
      "test accuracy: 0.525  ( 0.0707 )\n",
      "test f1: 0.4343  ( 0.1253 )\n",
      "Basic average compressions:  10.236970947265625\n",
      "Threshold:  12\n",
      "test accuracy: 0.55  ( 0.1214 )\n",
      "test f1: 0.4474  ( 0.1954 )\n",
      "Basic average compressions:  12.125520751953125\n",
      "Threshold:  14\n",
      "test accuracy: 0.45  ( 0.0758 )\n",
      "test f1: 0.3325  ( 0.1133 )\n",
      "Basic average compressions:  14.016361328125\n",
      "Threshold:  16\n",
      "test accuracy: 0.455  ( 0.1077 )\n",
      "test f1: 0.3478  ( 0.1226 )\n",
      "Basic average compressions:  16.297826904296876\n",
      "Threshold:  18\n",
      "test accuracy: 0.46  ( 0.0561 )\n",
      "test f1: 0.3252  ( 0.1043 )\n",
      "Basic average compressions:  18.189426513671876\n",
      "Threshold:  20\n",
      "test accuracy: 0.365  ( 0.0718 )\n",
      "test f1: 0.3309  ( 0.0852 )\n",
      "Basic average compressions:  20.08026318359375\n",
      "Threshold:  22\n",
      "test accuracy: 0.46  ( 0.0561 )\n",
      "test f1: 0.3252  ( 0.1043 )\n",
      "Basic average compressions:  21.971101806640625\n",
      "Threshold:  24\n",
      "test accuracy: 0.4  ( 0.1037 )\n",
      "test f1: 0.2661  ( 0.0629 )\n",
      "Basic average compressions:  24.25088891601563\n",
      "Threshold:  26\n",
      "test accuracy: 0.39  ( 0.0464 )\n",
      "test f1: 0.2528  ( 0.0261 )\n",
      "Basic average compressions:  26.141115234375\n",
      "Threshold:  28\n",
      "test accuracy: 0.43  ( 0.062 )\n",
      "test f1: 0.2956  ( 0.0726 )\n",
      "Basic average compressions:  28.031951904296875\n",
      "Threshold:  30\n",
      "test accuracy: 0.44  ( 0.0561 )\n",
      "test f1: 0.2709  ( 0.0607 )\n",
      "Basic average compressions:  29.923553466796875\n",
      "Threshold:  32\n",
      "test accuracy: 0.44  ( 0.0561 )\n",
      "test f1: 0.2709  ( 0.0607 )\n",
      "Basic average compressions:  32.20502099609375\n",
      "Threshold:  34\n",
      "test accuracy: 0.44  ( 0.0561 )\n",
      "test f1: 0.2709  ( 0.0607 )\n",
      "Basic average compressions:  34.092803955078125\n",
      "Threshold:  36\n",
      "test accuracy: 0.43  ( 0.066 )\n",
      "test f1: 0.3312  ( 0.078 )\n",
      "Basic average compressions:  35.984409423828126\n",
      "Threshold:  38\n",
      "test accuracy: 0.44  ( 0.0561 )\n",
      "test f1: 0.2709  ( 0.0607 )\n",
      "Basic average compressions:  37.875244140625\n",
      "Threshold:  40\n",
      "test accuracy: 0.44  ( 0.0561 )\n",
      "test f1: 0.2709  ( 0.0607 )\n",
      "Basic average compressions:  40.156707763671875\n",
      "Threshold:  42\n",
      "test accuracy: 0.44  ( 0.0561 )\n",
      "test f1: 0.2709  ( 0.0607 )\n",
      "Basic average compressions:  42.048309326171875\n",
      "Threshold:  44\n",
      "test accuracy: 0.44  ( 0.0561 )\n",
      "test f1: 0.2709  ( 0.0607 )\n",
      "Basic average compressions:  43.93914794921875\n",
      "Threshold:  46\n",
      "test accuracy: 0.44  ( 0.0561 )\n",
      "test f1: 0.2709  ( 0.0607 )\n",
      "Basic average compressions:  45.826934814453125\n",
      "Threshold:  48\n",
      "test accuracy: 0.44  ( 0.0561 )\n",
      "test f1: 0.2709  ( 0.0607 )\n",
      "Basic average compressions:  48.109161376953125\n",
      "Threshold:  50\n",
      "test accuracy: 0.44  ( 0.0561 )\n",
      "test f1: 0.2709  ( 0.0607 )\n",
      "Basic average compressions:  50.0\n",
      "Threshold:  52\n",
      "test accuracy: 0.44  ( 0.0561 )\n",
      "test f1: 0.2709  ( 0.0607 )\n",
      "Basic average compressions:  51.890838623046875\n",
      "Threshold:  54\n",
      "test accuracy: 0.44  ( 0.0561 )\n",
      "test f1: 0.2709  ( 0.0607 )\n",
      "Basic average compressions:  54.173065185546875\n",
      "Threshold:  56\n",
      "test accuracy: 0.41  ( 0.0875 )\n",
      "test f1: 0.2756  ( 0.0601 )\n",
      "Basic average compressions:  56.06085205078125\n",
      "Threshold:  58\n",
      "test accuracy: 0.44  ( 0.0561 )\n",
      "test f1: 0.2709  ( 0.0607 )\n",
      "Basic average compressions:  57.951690673828125\n",
      "Threshold:  60\n",
      "test accuracy: 0.44  ( 0.0561 )\n",
      "test f1: 0.2709  ( 0.0607 )\n",
      "Basic average compressions:  59.843292236328125\n",
      "Threshold:  62\n",
      "test accuracy: 0.44  ( 0.0561 )\n",
      "test f1: 0.2709  ( 0.0607 )\n",
      "Basic average compressions:  62.124755859375\n",
      "Threshold:  64\n",
      "test accuracy: 0.425  ( 0.0671 )\n",
      "test f1: 0.2908  ( 0.0677 )\n",
      "Basic average compressions:  64.01559448242188\n",
      "Threshold:  66\n",
      "test accuracy: 0.39  ( 0.0464 )\n",
      "test f1: 0.2528  ( 0.0261 )\n",
      "Basic average compressions:  65.90719995117188\n",
      "Threshold:  68\n",
      "test accuracy: 0.44  ( 0.0561 )\n",
      "test f1: 0.2709  ( 0.0607 )\n",
      "Basic average compressions:  67.79498486328126\n",
      "Threshold:  70\n",
      "test accuracy: 0.38  ( 0.0367 )\n",
      "test f1: 0.2774  ( 0.0513 )\n",
      "Basic average compressions:  70.07644848632813\n",
      "Threshold:  72\n",
      "test accuracy: 0.44  ( 0.0561 )\n",
      "test f1: 0.2709  ( 0.0607 )\n",
      "Basic average compressions:  71.96804809570312\n",
      "Threshold:  74\n",
      "test accuracy: 0.44  ( 0.0561 )\n",
      "test f1: 0.2709  ( 0.0607 )\n",
      "Basic average compressions:  73.85888671875\n",
      "Threshold:  76\n",
      "test accuracy: 0.41  ( 0.0875 )\n",
      "test f1: 0.2756  ( 0.0601 )\n",
      "Basic average compressions:  75.74972534179688\n",
      "Threshold:  78\n",
      "test accuracy: 0.44  ( 0.0561 )\n",
      "test f1: 0.2709  ( 0.0607 )\n",
      "Basic average compressions:  78.02890014648438\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "dbName = \"dnn/\"+dbName +\"/\"\n",
    "print(dbName)\n",
    "\n",
    "info = []\n",
    "basic_f1s = []\n",
    "basic_compressions = []\n",
    "basic_stds = []\n",
    "\n",
    "for threshold in range(2, 80,2):\n",
    "    print(\"Threshold: \",threshold)\n",
    "    #training basic AE\n",
    "    model_compressions = []\n",
    "    BFT_Error, basic_train_accuracy, basic_test_accuracy, basic_accuracy, basic_f1, basic_std = ae_DNN_training('basic',data_df, num_of_epoch, k_folds, threshold)\n",
    "    print('Basic average compressions: ', np.mean(model_compressions))\n",
    "    basic_f1s.append(basic_f1)\n",
    "    basic_compressions.append(np.mean(model_compressions))\n",
    "    basic_stds.append(basic_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aae1bbae-181f-47c6-9d5d-864108b04832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(dbName+'compressions.pkl', 'wb') as f:\n",
    "    pickle.dump(basic_f1s, f)\n",
    "    pickle.dump(basic_compressions, f)\n",
    "    pickle.dump(basic_stds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c8778f-9356-4261-9468-cfa9b51c46a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
