{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "canadian-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import csv\n",
    "import copy\n",
    "\n",
    "from math import log\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "from hanMaskingPackage import weight_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eb9ee60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# device = \"cpu\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2252e19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f84f8063810>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_folds = 5\n",
    "num_of_epoch = 100\n",
    "output_size = 1\n",
    "binary_classification = True\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1d077f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_Layer_1 = 256\n",
    "hidden_Layer_2 = 128\n",
    "hidden_Layer_3 = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddea7075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(train_df, test_df, y_column_name):\n",
    "    y_train = train_df[y_column_name]\n",
    "    X_train = train_df.drop([y_column_name], axis=1)\n",
    "\n",
    "    y_test = test_df[y_column_name]\n",
    "    X_test = test_df.drop([y_column_name], axis=1)\n",
    "\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    scale = min_max_scaler.fit(X_train)\n",
    "    \n",
    "    x_train_transformed = scale.transform(X_train)\n",
    "    x_test_transformed = scale.transform(X_test)\n",
    "\n",
    "    train_df_standardized = pd.DataFrame(x_train_transformed, columns = X_train.columns)\n",
    "    test_df_standardized = pd.DataFrame(x_test_transformed, columns = X_test.columns)\n",
    "\n",
    "    train_df = pd.concat([train_df_standardized.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
    "    test_df = pd.concat([test_df_standardized.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07b17843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_dataset(train_df, test_df, y_column_name):\n",
    "\n",
    "    y_train = train_df[y_column_name]\n",
    "    X_train = train_df.drop([y_column_name], axis=1)\n",
    "\n",
    "    y_test = test_df[y_column_name]\n",
    "    X_test = test_df.drop([y_column_name], axis=1)\n",
    "\n",
    "    # standardize\n",
    "    sc = StandardScaler()\n",
    "    scale = sc.fit(X_train)\n",
    "    \n",
    "    x_train_transformed = scale.transform(X_train)\n",
    "    x_test_transformed = scale.transform(X_test)\n",
    "\n",
    "    train_df_standardized = pd.DataFrame(x_train_transformed, columns = X_train.columns)\n",
    "    \n",
    "    test_df_standardized = pd.DataFrame(x_test_transformed, columns = X_test.columns)\n",
    "\n",
    "    train_df = pd.concat([train_df_standardized.reset_index(drop=True), y_train.reset_index(drop=True)], axis=1)\n",
    "    test_df = pd.concat([test_df_standardized.reset_index(drop=True), y_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cf93e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read files\n",
    "def file_reader(file_path):\n",
    "    '''Input = file path (str)\n",
    "       Output = numpy array of items in files\n",
    "    '''\n",
    "    \n",
    "    data = []\n",
    "    with open(file_path) as f:\n",
    "        reader = csv.reader(f, delimiter='\\n')\n",
    "        for row in reader:\n",
    "            for x in row:\n",
    "                x=x.split(' ')\n",
    "                example = []\n",
    "                for item in x:\n",
    "                    if item:\n",
    "                        item = int(item) #convert to int\n",
    "                        example.append(item)\n",
    "                data.append(example)\n",
    "        data = np.asarray(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3385bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## arcene\n",
    "dbName = 'arcene'\n",
    "\n",
    "arcene_train_X = file_reader('../hd-datasets/ARCENE/arcene_train.data')\n",
    "arcene_test_X = file_reader('../hd-datasets/ARCENE/arcene_valid.data')\n",
    "\n",
    "arcene_train_y = file_reader('../hd-datasets/ARCENE/arcene_train.labels')\n",
    "arcene_train_y = np.ravel(arcene_train_y)\n",
    "arcene_test_y = file_reader('../hd-datasets/ARCENE/arcene_valid.labels')\n",
    "arcene_test_y = np.ravel(arcene_test_y)\n",
    "\n",
    "arcene_train = np.column_stack( (arcene_train_X,arcene_train_y) )\n",
    "arcene_test = np.column_stack( (arcene_test_X,arcene_test_y) )\n",
    "arcene = np.row_stack( (arcene_train,arcene_test) )\n",
    "\n",
    "data_df = pd.DataFrame.from_records(arcene)\n",
    "y_column_name = 10000\n",
    "\n",
    "le = LabelEncoder()\n",
    "data_df[y_column_name] = le.fit_transform(data_df[y_column_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e33dbab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9991</th>\n",
       "      <th>9992</th>\n",
       "      <th>9993</th>\n",
       "      <th>9994</th>\n",
       "      <th>9995</th>\n",
       "      <th>9996</th>\n",
       "      <th>9997</th>\n",
       "      <th>9998</th>\n",
       "      <th>9999</th>\n",
       "      <th>10000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>538</td>\n",
       "      <td>404</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>570</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>524</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>82</td>\n",
       "      <td>165</td>\n",
       "      <td>60</td>\n",
       "      <td>554</td>\n",
       "      <td>379</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>605</td>\n",
       "      <td>69</td>\n",
       "      <td>7</td>\n",
       "      <td>473</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>423</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>451</td>\n",
       "      <td>402</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>593</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>508</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>44</td>\n",
       "      <td>275</td>\n",
       "      <td>14</td>\n",
       "      <td>511</td>\n",
       "      <td>470</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>600</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>469</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>348</td>\n",
       "      <td>0</td>\n",
       "      <td>268</td>\n",
       "      <td>329</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>301</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>24</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>436</td>\n",
       "      <td>92</td>\n",
       "      <td>400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>139</td>\n",
       "      <td>261</td>\n",
       "      <td>...</td>\n",
       "      <td>540</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>130</td>\n",
       "      <td>365</td>\n",
       "      <td>58</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>11</td>\n",
       "      <td>58</td>\n",
       "      <td>50</td>\n",
       "      <td>332</td>\n",
       "      <td>109</td>\n",
       "      <td>393</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>134</td>\n",
       "      <td>...</td>\n",
       "      <td>355</td>\n",
       "      <td>156</td>\n",
       "      <td>77</td>\n",
       "      <td>26</td>\n",
       "      <td>277</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>261</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>93</td>\n",
       "      <td>32</td>\n",
       "      <td>137</td>\n",
       "      <td>319</td>\n",
       "      <td>0</td>\n",
       "      <td>264</td>\n",
       "      <td>231</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>244</td>\n",
       "      <td>309</td>\n",
       "      <td>0</td>\n",
       "      <td>276</td>\n",
       "      <td>312</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>119</td>\n",
       "      <td>12</td>\n",
       "      <td>198</td>\n",
       "      <td>339</td>\n",
       "      <td>0</td>\n",
       "      <td>289</td>\n",
       "      <td>410</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>256</td>\n",
       "      <td>402</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>350</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>112</td>\n",
       "      <td>19</td>\n",
       "      <td>171</td>\n",
       "      <td>334</td>\n",
       "      <td>0</td>\n",
       "      <td>282</td>\n",
       "      <td>208</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>226</td>\n",
       "      <td>379</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>367</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 10001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0      1      2      3      4      5      6      7      8      9      \\\n",
       "0        0     71      0     95      0    538    404     20      0      0   \n",
       "1        0     41     82    165     60    554    379      0     71      0   \n",
       "2        0      0      1     40      0    451    402      0      0      0   \n",
       "3        0     56     44    275     14    511    470      0      0      0   \n",
       "4      105      0    141    348      0    268    329      0      0      1   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "195     24     73      0    436     92    400      0      0    139    261   \n",
       "196     11     58     50    332    109    393    122      0     75    134   \n",
       "197     93     32    137    319      0    264    231     21      0      0   \n",
       "198    119     12    198    339      0    289    410      0      0      4   \n",
       "199    112     19    171    334      0    282    208      0      0      0   \n",
       "\n",
       "     ...  9991   9992   9993   9994   9995   9996   9997   9998   9999   10000  \n",
       "0    ...    570     86      0     36      0     80      0      0    524      1  \n",
       "1    ...    605     69      7    473      0     57      0    284    423      0  \n",
       "2    ...    593     28      0     24      0     90      0     34    508      1  \n",
       "3    ...    600      0     26     86      0    102      0      0    469      1  \n",
       "4    ...      0      0      0      0    190    301      0      0    354      0  \n",
       "..   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "195  ...    540      0     86    130    365     58     17      3     37      0  \n",
       "196  ...    355    156     77     26    277    265      0     36    261      0  \n",
       "197  ...      9      0      0      0    244    309      0    276    312      1  \n",
       "198  ...      0     37      0      0    256    402      0      0    350      1  \n",
       "199  ...      0    118      0      0    226    379      0      0    367      0  \n",
       "\n",
       "[200 rows x 10001 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe6806ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_multiple_lists(multiple_lists):\n",
    "    data = np.array(multiple_lists)\n",
    "    return np.average(data, axis=0), np.std(data,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1aabea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic auto encoder with one layer\n",
    "class Autoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size=24,output_size=12):\n",
    "        super().__init__()       \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, output_size),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.20)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(output_size, input_size),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.20)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.encoded = self.encoder(x)\n",
    "        decoded = self.decoder(self.encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf2c8f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic auto encoder with three layers\n",
    "class Basic_Autoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size = 1):\n",
    "\n",
    "        hidden_L_1 = hidden_Layer_1\n",
    "        hidden_L_2 = hidden_Layer_2\n",
    "        hidden_L_3 = hidden_Layer_3\n",
    "        \n",
    "\n",
    "        super().__init__()        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_L_1),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.20),\n",
    "            nn.Linear(hidden_L_1, hidden_L_2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.20),\n",
    "            nn.Linear(hidden_L_2, hidden_L_3),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.20)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_L_3, hidden_L_2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.20),\n",
    "            nn.Linear(hidden_L_2, hidden_L_1),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.20),\n",
    "            nn.Linear(hidden_L_1, input_size),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.20)\n",
    "        )\n",
    "\n",
    "    # pre training\n",
    "    def forward(self, x):\n",
    "        self.encoded = self.encoder(x)\n",
    "        decoded = self.decoder(self.encoded)\n",
    "        return decoded\n",
    "\n",
    "class Basic_Autoencoder_Classifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size, encoder):\n",
    "        \n",
    "        hidden_L_3 = hidden_Layer_3\n",
    "        \n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classification_layer = nn.Linear(hidden_L_3, output_size)\n",
    "        self.classifier = nn.Sequential(self.encoder, self.classification_layer)\n",
    "        self.activation = nn.Softmax(dim=1)\n",
    "        if binary_classification:\n",
    "            self.activation = nn.Sigmoid()\n",
    "\n",
    "    # fine-tuning\n",
    "    def forward(self, x):\n",
    "        intermediate = self.classifier(x)\n",
    "        if binary_classification:\n",
    "            return self.activation(intermediate)\n",
    "        else:\n",
    "            # no activation function is used for multiclass classification\n",
    "            # cross entrophy loss automatically applies softmax in pytorch\n",
    "            return intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdd91386",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fusion_Autoencoder(nn.Module):\n",
    "    def __init__(self,input_size, output_size=1):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_L_1 = hidden_Layer_1\n",
    "        hidden_L_2 = hidden_Layer_2\n",
    "        hidden_L_3 = hidden_Layer_3\n",
    "\n",
    "        super().__init__()        \n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_L_1),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.20)\n",
    "        )\n",
    "\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Linear(hidden_L_1, hidden_L_2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.20),\n",
    "        )\n",
    "\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_L_1 + hidden_L_2, hidden_L_3),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.20)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_L_3, hidden_L_2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.20),\n",
    "            nn.Linear(hidden_L_2, hidden_L_1),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.20),\n",
    "            nn.Linear(hidden_L_1, input_size),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.20)\n",
    "        )\n",
    "    \n",
    "    # pre-training\n",
    "    def forward(self, x):\n",
    "        intermediate_1 = self.encoder1(x)\n",
    "        intermediate_2 = self.encoder2((intermediate_1))\n",
    "        fusion_tensor = torch.cat((intermediate_1, intermediate_2), 1)\n",
    "        self.encoded = self.fusion(fusion_tensor)\n",
    "        decoded = self.decoder(self.encoded)\n",
    "        return decoded\n",
    "\n",
    "class Fusion_Autoencoder_Classifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        hidden_L_1 = hidden_Layer_1\n",
    "        hidden_L_2 = hidden_Layer_2\n",
    "        hidden_L_3 = hidden_Layer_3\n",
    "\n",
    "        super().__init__()        \n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_L_1),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.20)\n",
    "        )\n",
    "\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Linear(hidden_L_1, hidden_L_2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.20),\n",
    "        )\n",
    "\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_L_1 + hidden_L_2, hidden_L_3),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.20)\n",
    "        )\n",
    "\n",
    "        self.classification_layer = nn.Linear(hidden_L_3, output_size)\n",
    "        self.activation = nn.Softmax(dim=1)\n",
    "        if binary_classification:\n",
    "            self.activation = nn.Sigmoid()\n",
    "    \n",
    "    # fine-tuning\n",
    "    def forward(self, x):\n",
    "        intermediate_1 = self.encoder1(x)\n",
    "        intermediate_2 = self.encoder2((intermediate_1))\n",
    "        fusion_tensor = torch.cat((intermediate_1, intermediate_2), 1)\n",
    "        self.encoded = self.fusion(fusion_tensor)\n",
    "        intermediate = self.classification_layer(self.encoded)\n",
    "        \n",
    "        if binary_classification:\n",
    "            return self.activation(intermediate)\n",
    "        else:\n",
    "            # no activation function is used for multiclass classification\n",
    "            # cross entrophy loss automatically applies softmax in pytorch\n",
    "            return intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c80950f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fusion_Stacked_Autoencoder(nn.Module):\n",
    "    def __init__(self,input_size, output_size=1):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_L_1 = hidden_Layer_1\n",
    "        hidden_L_2 = hidden_Layer_2\n",
    "        hidden_L_3 = hidden_Layer_3 \n",
    "\n",
    "        self.ae1 = Autoencoder(input_size=input_size, output_size=hidden_L_1)\n",
    "        self.ae2 = Autoencoder(input_size=hidden_L_1, output_size=hidden_L_2)\n",
    "        self.ae3 = Autoencoder(input_size=hidden_L_2, output_size=hidden_L_3)\n",
    "        self.ae_fusion = Autoencoder(input_size=hidden_L_1+hidden_L_2, output_size=hidden_L_3)\n",
    "        \n",
    "        self.classification_layer = nn.Linear(hidden_L_3, output_size)\n",
    "        # Sigmoid activation is used for binary classification, softmax for multiclass\n",
    "        self.activation = nn.Softmax(dim=1)\n",
    "        if binary_classification:\n",
    "            self.activation = nn.Sigmoid()\n",
    "\n",
    "        # add the 3 encoders and decoders in a sequential\n",
    "        # self.encoder_model = nn.Sequential(self.ae1.encoder, self.ae2.encoder, self.ae3.encoder)\n",
    "        # self.classifier = nn.Sequential(self.encoder_model, self.classification_layer)\n",
    "        self.decoder_model = nn.Sequential(self.ae3.decoder, self.ae2.decoder, self.ae1.decoder)\n",
    "    \n",
    "    # pre-training\n",
    "    def forward(self, x):\n",
    "        intermediate_1 = self.ae1.encoder(x)\n",
    "        intermediate_2 = self.ae2.encoder((intermediate_1))\n",
    "\n",
    "        fusion_tensor = torch.cat((intermediate_1, intermediate_2), 1)\n",
    "        fusion_tensor = self.ae_fusion.encoder(fusion_tensor)\n",
    "        intermediate_3 = self.ae3.encoder(intermediate_2)\n",
    "        \n",
    "        self.encoded = torch.div(fusion_tensor.add(intermediate_3), 2)\n",
    "        intermediate = self.classification_layer(self.encoded)\n",
    "        if binary_classification:\n",
    "            return self.activation(intermediate)\n",
    "        else:\n",
    "            # no activation function is used for multiclass classification\n",
    "            # cross entrophy loss automatically applies softmax in pytorch\n",
    "            return intermediate\n",
    "\n",
    "class Fusion_Stacked_Autoencoder_Classifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size, ae1, ae2, ae3, ae_fusion):\n",
    "        hidden_L_3 = hidden_Layer_3\n",
    "        \n",
    "        super().__init__()\n",
    "        self.ae1 = ae1\n",
    "        self.ae2 = ae2\n",
    "        self.ae3 = ae3\n",
    "        self.ae_fusion = ae_fusion\n",
    "\n",
    "        self.classification_layer = nn.Linear(hidden_L_3, output_size)\n",
    "        self.activation = nn.Softmax(dim=1)\n",
    "        if binary_classification:\n",
    "            self.activation = nn.Sigmoid()\n",
    "\n",
    "    # fine-tuning\n",
    "    def forward(self, x):\n",
    "        intermediate_1 = self.ae1.encoder(x)\n",
    "        intermediate_2 = self.ae2.encoder((intermediate_1))\n",
    "\n",
    "        fusion_tensor = torch.cat((intermediate_1, intermediate_2), 1)\n",
    "        fusion_tensor = self.ae_fusion.encoder(fusion_tensor)\n",
    "        intermediate_3 = self.ae3.encoder(intermediate_2)\n",
    "        \n",
    "        self.encoded = torch.div(fusion_tensor.add(intermediate_3), 2)\n",
    "        intermediate = self.classification_layer(self.encoded)\n",
    "        \n",
    "        if binary_classification:\n",
    "            return self.activation(intermediate)\n",
    "        else:\n",
    "            # no activation function is used for multiclass classification\n",
    "            # cross entrophy loss automatically applies softmax in pytorch\n",
    "            return intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80e1cf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stacked_Autoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size, output_size=1):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_L_1 = hidden_Layer_1\n",
    "        hidden_L_2 = hidden_Layer_2\n",
    "        hidden_L_3 = hidden_Layer_3     \n",
    "\n",
    "        self.ae1 = Autoencoder(input_size=input_size, output_size=hidden_L_1)\n",
    "        self.ae2 = Autoencoder(input_size=hidden_L_1, output_size=hidden_L_2)\n",
    "        self.ae3 = Autoencoder(input_size=hidden_L_2, output_size=hidden_L_3)\n",
    "\n",
    "        # add the 3 encoders and decoders in a sequential\n",
    "        self.encoder = nn.Sequential(self.ae1.encoder, self.ae2.encoder, self.ae3.encoder)\n",
    "        self.decoder = nn.Sequential(self.ae3.decoder, self.ae2.decoder, self.ae1.decoder)\n",
    "\n",
    "    # pre-training\n",
    "    def forward(self, x):\n",
    "        self.encoded = self.encoder(x)\n",
    "        decoded = self.decoder(self.encoded)\n",
    "        return decoded\n",
    "\n",
    "class Stacked_Autoencoder_Classifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size, encoder):\n",
    "        hidden_L_3 = hidden_Layer_3\n",
    "        \n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classification_layer = nn.Linear(hidden_L_3, output_size)\n",
    "        self.classifier = nn.Sequential(self.encoder, self.classification_layer)\n",
    "        self.activation = nn.Softmax(dim=1)\n",
    "        if binary_classification:\n",
    "            self.activation = nn.Sigmoid()\n",
    "\n",
    "    # fine-tuning\n",
    "    def forward(self, x):\n",
    "        intermediate = self.classifier(x)\n",
    "        if binary_classification:\n",
    "            return self.activation(intermediate)\n",
    "        else:\n",
    "            # no activation function is used for multiclass classification\n",
    "            # cross entrophy loss automatically applies softmax in pytorch\n",
    "            return intermediate\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edfa74ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_perturbation(model, epoch, perc_weight, threshold, Wold_dict):\n",
    "\n",
    "    state_dict = model.state_dict()\n",
    "    \n",
    "\n",
    "    for name, param in state_dict.items():\n",
    "\n",
    "        # ignore biases and the decoder weights\n",
    "        if not \"weight\" in name or \"decoder\" in name:\n",
    "            continue\n",
    "\n",
    "\n",
    "        W =  param.cpu().numpy()\n",
    "        if name not in Wold_dict:\n",
    "            Wold = np.ones(W.shape)\n",
    "        else:\n",
    "            Wold = Wold_dict[name]\n",
    "\n",
    "        Wold, perc = weight_perc(Wold,W,threshold)\n",
    "        Wold_dict[name] = Wold\n",
    "        perc_weight.append(perc)\n",
    "        wm = np.multiply(Wold,W)\n",
    "        wm = torch.from_numpy(wm)\n",
    "        state_dict[name].copy_(wm)\n",
    "\n",
    "        # print('The average of weights perturbation: ',sum(perc_weight)/len(perc_weight), '%')\n",
    "\n",
    "    return perc_weight, Wold_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db183561-aad9-4d7f-8114-82774f54ef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_weight_masking(model, Wold_dict):\n",
    "\n",
    "    state_dict = model.state_dict()\n",
    "    \n",
    "\n",
    "    for name, param in state_dict.items():\n",
    "\n",
    "        # ignore biases and the decoder weights\n",
    "        if not \"weight\" in name or \"decoder\" in name:\n",
    "            continue\n",
    "\n",
    "\n",
    "        W =  param.cpu().numpy()\n",
    "        if name not in Wold_dict:\n",
    "            Wold = np.ones(W.shape)\n",
    "        else:\n",
    "            Wold = Wold_dict[name]\n",
    "\n",
    "        Wold_dict[name] = Wold\n",
    "        wm = np.multiply(Wold,W)\n",
    "        wm = torch.from_numpy(wm)\n",
    "        state_dict[name].copy_(wm)\n",
    "\n",
    "        # print('The average of weights perturbation: ',sum(perc_weight)/len(perc_weight), '%')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e53aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compressions = []\n",
    "\n",
    "def calculate_model_compression(model):\n",
    "    perc_weight = []\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    for name, param in state_dict.items():\n",
    "\n",
    "        # ignore biases and the decoder weights\n",
    "        if not \"weight\" in name or \"decoder\" in name:\n",
    "            continue\n",
    "\n",
    "        W =  param.cpu().numpy()\n",
    "        cnt_zero = len(np.ravel(W))-np.count_nonzero(W)\n",
    "        perc = (( cnt_zero)*100)/len(np.ravel(W))\n",
    "        perc_weight.append(perc)\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model_compressions.append(sum(perc_weight)/len(perc_weight))\n",
    "    return model\n",
    "    # print('Achieved model compression: ',sum(perc_weight)/len(perc_weight), '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a5362b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pretraining(model, dataset, num_of_epoch, threshold):\n",
    "\n",
    "    # to run on gpu\n",
    "    model = model.to(device)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                            batch_size=64,\n",
    "                                            shuffle=True)\n",
    "\n",
    "    # latent is needed for stacked autoencoder pre-training\n",
    "    latent = torch.tensor([])\n",
    "    latent = latent.to(device)\n",
    "    \n",
    "    training_lle = []\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    minimum_loss = None\n",
    "    best_epoch = 0\n",
    "\n",
    "    # perturbation variables\n",
    "    perc_weight = []\n",
    "    Wold_dict = {}\n",
    "\n",
    "    for epoch in range(num_of_epoch):\n",
    "        for train in data_loader:\n",
    "            model = model.train()\n",
    "            optimizer.zero_grad()\n",
    "            train = train.float()\n",
    "            decoded = model(train)\n",
    "            loss = criterion(decoded, train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            if(epoch == num_of_epoch - 1):\n",
    "                latent = torch.cat((latent, model.encoded), 0)\n",
    "            \n",
    "            # save the model with minimum loss\n",
    "            # if minimum_loss is None or minimum_loss > loss.item():\n",
    "            #     minimum_loss = loss.item()\n",
    "            #     best_epoch = epoch\n",
    "            #     torch.save(model.state_dict(), dbName+'pretrained_best_model.pt')\n",
    "\n",
    "        training_lle.append(loss.item())\n",
    "        \n",
    "        # weight perturbation happens here\n",
    "    perc_weight, Wold_dict = weight_perturbation(model, epoch, perc_weight, threshold, Wold_dict)\n",
    "    \n",
    "\n",
    "\n",
    "    # load and return the model with minimum loss at the end\n",
    "    # print(\"best_epoch = \" + str(best_epoch))\n",
    "    # model.load_state_dict(torch.load(dbName+'pretrained_best_model.pt'))\n",
    "    model = calculate_model_compression(model)\n",
    "    \n",
    "    return model, training_lle, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8f0a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_finetuning(model, train_tensor, num_of_epoch, train_x, train_y, test_x, test_y):\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    finetuning_loss = []\n",
    "    train_accuracy_scores = []\n",
    "    test_accuracy_scores = []\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=train_tensor,\n",
    "                                        batch_size=64,\n",
    "                                        shuffle=True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if binary_classification:\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    perc_weight = []\n",
    "\n",
    "    limit_counter = 0\n",
    "    for epoch in range(num_of_epoch):\n",
    "        model = model.train()\n",
    "        for train,target in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # to make sure all variables are float type\n",
    "            train = train.float()\n",
    "            target = target.float()\n",
    "\n",
    "            prediction = model(train)\n",
    "            # target = target.unsqueeze(1)\n",
    "            classification_loss = criterion(prediction, target.unsqueeze(1))\n",
    "            classification_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        finetuning_loss.append(classification_loss.item())\n",
    "\n",
    "        # train accuracy after each epoch\n",
    "        model = model.eval()\n",
    "        prediction_train = model(train_x)\n",
    "        prediction_train = prediction_train.flatten()\n",
    "        train_accuracy = accuracy_score(train_y.cpu().detach().numpy(),np.round(prediction_train.cpu().detach().numpy()))\n",
    "        train_accuracy_scores.append(train_accuracy)\n",
    "\n",
    "        # test accuracy after each epoch\n",
    "        prediction_test = model(test_x)\n",
    "        prediction_test = prediction_test.flatten()\n",
    "        test_accuracy = accuracy_score(test_y.cpu().detach().numpy(),np.round(prediction_test.cpu().detach().numpy()))\n",
    "        test_accuracy_scores.append(test_accuracy)\n",
    "    \n",
    "    return model, finetuning_loss, train_accuracy_scores ,test_accuracy_scores, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31bb74cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_tensor(df):\n",
    "    y_train = df[y_column_name]\n",
    "    X_train = df.drop([y_column_name], axis=1)\n",
    "    y_tensor = torch.tensor(y_train.values)\n",
    "    x_tensor = torch.tensor(X_train.values)\n",
    "    x_tensor = x_tensor.to(device)\n",
    "    y_tensor = y_tensor.to(device)\n",
    "\n",
    "    # commented out normalization here because data is standardized\n",
    "    # x_tensor_norm = torch.nn.functional.normalize(x_tensor, p=2.0, dim=1, eps=1e-12, out=None)\n",
    "    \n",
    "    dataset_tensor = data_utils.TensorDataset(x_tensor, y_tensor)\n",
    "    return x_tensor, y_tensor, dataset_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d2e5f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_training(model_name, dataset, num_of_epoch, k_folds, threshold):\n",
    "\n",
    "    kfold = KFold(n_splits = k_folds, random_state= 42, shuffle = True)\n",
    "\n",
    "    \n",
    "    pretraining_loss_fold = []\n",
    "    pretraining_loss_1_fold = []\n",
    "    pretraining_loss_2_fold = []\n",
    "    pretraining_loss_3_fold = []\n",
    "    finetuning_loss_fold = []\n",
    "    train_accuracy_fold = []\n",
    "    test_accuracy_fold = []\n",
    "    classification_accuracy_fold = []\n",
    "    f1_fold = []\n",
    "    \n",
    "    fold_counter = 0\n",
    "    # run k fold in loop\n",
    "    for train, test in list(kfold.split(dataset)):\n",
    "\n",
    "        fold_counter+=1\n",
    "\n",
    "        # divide the data for train and test\n",
    "        train_df = dataset.iloc[train]\n",
    "        test_df =  dataset.iloc[test]\n",
    "\n",
    "        train_df, test_df = standardize_dataset(train_df, test_df, y_column_name)\n",
    "        # train_df= normalize_dataset(train_df, y_column_name)\n",
    "        # test_df= normalize_dataset(test_df, y_column_name)\n",
    "\n",
    "        # convert dataframe into train and test tensor\n",
    "        train_x, train_y, train_tensor = convert_df_to_tensor(train_df)\n",
    "        test_x, test_y, test_tensor = convert_df_to_tensor(test_df)\n",
    "\n",
    "        x_dim = train_x.shape[1]\n",
    "        \n",
    "        model = None\n",
    "        if model_name == \"basic\" or model_name == \"fusion\":\n",
    "            if model_name == \"basic\":                                          \n",
    "                model = Basic_Autoencoder(input_size=x_dim)\n",
    "            \n",
    "            # pretraining done here\n",
    "            model, training_lle, latent = model_pretraining(model, train_x.float(), num_of_epoch, threshold)\n",
    "\n",
    "            \n",
    "            if model_name == \"basic\":\n",
    "                # copy the trained weights and biases when classifier is added to model\n",
    "                # deep copy is recommended when copying weights and biases from a model\n",
    "                encoder_sd = copy.deepcopy(model.encoder.state_dict())\n",
    "                model = Basic_Autoencoder_Classifier(x_dim, output_size, model.encoder)\n",
    "                model.encoder.load_state_dict(encoder_sd)\n",
    "                model.encoder.requires_grad_(False)\n",
    "\n",
    "            # finetuning done here\n",
    "            model, finetuning_loss, train_accuracy, test_accuracy, prediction = model_finetuning(model, train_tensor, num_of_epoch, train_x.float(), train_y.float(), test_x.float(), test_y.float())\n",
    "\n",
    "            pretraining_loss_fold.append(training_lle)\n",
    "            finetuning_loss_fold.append(finetuning_loss)\n",
    "            train_accuracy_fold.append(train_accuracy)\n",
    "            test_accuracy_fold.append(test_accuracy)\n",
    "\n",
    "        elif model_name == \"stacked\":\n",
    "            model = Stacked_Autoencoder(input_size=x_dim)                                         \n",
    "\n",
    "            model.ae1, training_lle_1, latent_1 = model_pretraining(model.ae1, train_x.float(), num_of_epoch, threshold)\n",
    "            model.ae2, training_lle_2, latent_2 = model_pretraining(model.ae2, latent_1.detach(), num_of_epoch, threshold)\n",
    "            model.ae3, training_lle_3, latent_3 = model_pretraining(model.ae3, latent_2.detach(), num_of_epoch, threshold)\n",
    "\n",
    "\n",
    "            encoder_sd = copy.deepcopy(model.encoder.state_dict())\n",
    "            model = Stacked_Autoencoder_Classifier(x_dim, output_size, model.encoder)\n",
    "            model.encoder.load_state_dict(encoder_sd)\n",
    "            model.encoder.requires_grad_(False)\n",
    "\n",
    "                    \n",
    "            model, finetuning_loss, train_accuracy, test_accuracy, prediction = model_finetuning(model, train_tensor, num_of_epoch, train_x.float(), train_y.float(), test_x.float(), test_y.float())\n",
    "            \n",
    "            pretraining_loss_1_fold.append(training_lle_1)\n",
    "            pretraining_loss_2_fold.append(training_lle_2)\n",
    "            pretraining_loss_3_fold.append(training_lle_3)\n",
    "            finetuning_loss_fold.append(finetuning_loss)\n",
    "            train_accuracy_fold.append(train_accuracy)\n",
    "            test_accuracy_fold.append(test_accuracy)\n",
    "\n",
    "            latent = [latent_1, latent_2, latent_3]\n",
    "\n",
    "        # calculate final test accuracy, confusion matrix etc\n",
    "        test_x = test_x.float()\n",
    "        prediction_tensor = model(test_x)\n",
    "        classification_accuracy_fold.append(accuracy_score(test_y.cpu().detach().numpy(),np.round(prediction_tensor.cpu().detach().numpy().flatten())))\n",
    "        f1_fold.append(f1_score(test_y.cpu().detach().numpy(),np.round(prediction_tensor.cpu().detach().numpy().flatten()), average='weighted'))\n",
    "        report = classification_report(test_y.cpu().detach().numpy(),np.round_(prediction_tensor.cpu().detach().numpy().flatten()), output_dict=True)\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "        report_df.to_csv(dbName+\"_\"+model_name+\"_\"+str(fold_counter), index= True)\n",
    "\n",
    "        # take average of fold losses\n",
    "        if model_name == \"stacked\":\n",
    "            training_lle_1 = average_multiple_lists(pretraining_loss_1_fold)\n",
    "            training_lle_2 = average_multiple_lists(pretraining_loss_2_fold)\n",
    "            training_lle_3 = average_multiple_lists(pretraining_loss_3_fold)\n",
    "            training_lle = [training_lle_1, training_lle_2, training_lle_3]\n",
    "        else:\n",
    "            training_lle = average_multiple_lists(pretraining_loss_fold)\n",
    "            \n",
    "        finetuning_loss = average_multiple_lists(finetuning_loss_fold)\n",
    "        train_accuracy_curve = average_multiple_lists(train_accuracy_fold)\n",
    "        test_accuracy_curve = average_multiple_lists(test_accuracy_fold)\n",
    "\n",
    "    classification_accuracy_mean = np.round(statistics.mean(classification_accuracy_fold),4)\n",
    "    classification_accuracy_std = np.round(statistics.pstdev(classification_accuracy_fold),4)\n",
    "    print(\"test accuracy:\",classification_accuracy_mean,\" (\",classification_accuracy_std,\")\")\n",
    "\n",
    "    f1_mean = np.round(statistics.mean(f1_fold),4)\n",
    "    f1_std = np.round(statistics.pstdev(f1_fold),4)\n",
    "    print(\"test f1:\",f1_mean,\" (\",f1_std,\")\")\n",
    "    \n",
    "    return training_lle, finetuning_loss, train_accuracy_curve, test_accuracy_curve, latent, f1_mean, f1_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d404ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shaded_plot(pair, label, x_label, y_label, filename):\n",
    "    mean = pair[0]\n",
    "    std = pair[1]\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    x = np.arange(len(mean))\n",
    "    plt.plot(x, mean, label=label)\n",
    "    plt.fill_between(x, mean - std, mean + std, alpha=0.2)\n",
    "    plt.legend(prop={'size': 13})\n",
    "    plt.xlabel(x_label,fontsize=13)\n",
    "    plt.ylabel(y_label,fontsize=13)\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def shaded_plot_multiple(pairs, labels, x_label, y_label, filename):\n",
    "    \n",
    "    plt.figure(figsize=(9, 6))\n",
    "\n",
    "    for pair, label in zip(pairs, labels):\n",
    "        mean = pair[0]\n",
    "        std = pair[1]\n",
    "        x = np.arange(len(mean))\n",
    "        plt.plot(x, mean, label=label)\n",
    "        plt.fill_between(x, mean - std, mean + std, alpha=0.2)\n",
    "    \n",
    "    plt.legend(prop={'size': 13})\n",
    "    plt.xlabel(x_label,fontsize=13)\n",
    "    plt.ylabel(y_label,fontsize=13)\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87f52074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arcene\n",
      "Threshold:  2\n",
      "test accuracy: 0.765  ( 0.0374 )\n",
      "test f1: 0.763  ( 0.0397 )\n",
      "Basic average compressions:  2.0013020833333335\n",
      "test accuracy: 0.69  ( 0.0735 )\n",
      "test f1: 0.6875  ( 0.0753 )\n",
      "Stacked average compressions:  2.0013046875000002\n",
      "Threshold:  4\n",
      "test accuracy: 0.75  ( 0.0742 )\n",
      "test f1: 0.7457  ( 0.081 )\n",
      "Basic average compressions:  4.001592122395833\n",
      "test accuracy: 0.64  ( 0.0644 )\n",
      "test f1: 0.6377  ( 0.0666 )\n",
      "Stacked average compressions:  4.0015921223958335\n",
      "Threshold:  6\n",
      "test accuracy: 0.75  ( 0.0671 )\n",
      "test f1: 0.7461  ( 0.0728 )\n",
      "Basic average compressions:  6.0028942057291665\n",
      "test accuracy: 0.69  ( 0.0436 )\n",
      "test f1: 0.6856  ( 0.0434 )\n",
      "Stacked average compressions:  6.002899414062499\n",
      "Threshold:  8\n",
      "test accuracy: 0.695  ( 0.1054 )\n",
      "test f1: 0.6921  ( 0.1084 )\n",
      "Basic average compressions:  8.003181640625\n",
      "test accuracy: 0.625  ( 0.0387 )\n",
      "test f1: 0.6246  ( 0.0384 )\n",
      "Stacked average compressions:  8.003179036458334\n",
      "Threshold:  10\n",
      "test accuracy: 0.69  ( 0.086 )\n",
      "test f1: 0.6868  ( 0.0898 )\n",
      "Basic average compressions:  10.003458658854166\n",
      "test accuracy: 0.64  ( 0.0539 )\n",
      "test f1: 0.6375  ( 0.0545 )\n",
      "Stacked average compressions:  10.0034638671875\n",
      "Threshold:  12\n",
      "test accuracy: 0.695  ( 0.0696 )\n",
      "test f1: 0.6869  ( 0.0795 )\n",
      "Basic average compressions:  12.000699544270834\n",
      "test accuracy: 0.61  ( 0.097 )\n",
      "test f1: 0.5943  ( 0.1097 )\n",
      "Stacked average compressions:  12.000691731770834\n",
      "Threshold:  14\n",
      "test accuracy: 0.64  ( 0.1125 )\n",
      "test f1: 0.6353  ( 0.1185 )\n",
      "Basic average compressions:  14.000981770833334\n",
      "test accuracy: 0.66  ( 0.0583 )\n",
      "test f1: 0.6493  ( 0.0652 )\n",
      "Stacked average compressions:  14.0009765625\n",
      "Threshold:  16\n",
      "test accuracy: 0.705  ( 0.0828 )\n",
      "test f1: 0.7021  ( 0.0836 )\n",
      "Basic average compressions:  16.001261393229168\n",
      "test accuracy: 0.64  ( 0.0374 )\n",
      "test f1: 0.6371  ( 0.033 )\n",
      "Stacked average compressions:  16.001263997395835\n",
      "Threshold:  18\n",
      "test accuracy: 0.675  ( 0.0866 )\n",
      "test f1: 0.6753  ( 0.0868 )\n",
      "Basic average compressions:  18.0025634765625\n",
      "test accuracy: 0.64  ( 0.0752 )\n",
      "test f1: 0.6364  ( 0.0776 )\n",
      "Stacked average compressions:  18.00256608072917\n",
      "Threshold:  20\n",
      "test accuracy: 0.58  ( 0.043 )\n",
      "test f1: 0.5713  ( 0.0543 )\n",
      "Basic average compressions:  20.002848307291668\n",
      "test accuracy: 0.63  ( 0.06 )\n",
      "test f1: 0.6236  ( 0.0651 )\n",
      "Stacked average compressions:  20.002848307291668\n",
      "Threshold:  22\n",
      "test accuracy: 0.61  ( 0.0663 )\n",
      "test f1: 0.6102  ( 0.067 )\n",
      "Basic average compressions:  22.003133138020832\n",
      "test accuracy: 0.635  ( 0.0644 )\n",
      "test f1: 0.6243  ( 0.0724 )\n",
      "Stacked average compressions:  22.003133138020832\n",
      "Threshold:  24\n",
      "test accuracy: 0.665  ( 0.0604 )\n",
      "test f1: 0.6528  ( 0.0702 )\n",
      "Basic average compressions:  24.0003662109375\n",
      "test accuracy: 0.68  ( 0.0812 )\n",
      "test f1: 0.6807  ( 0.0799 )\n",
      "Stacked average compressions:  24.00036881510417\n",
      "Threshold:  26\n",
      "test accuracy: 0.615  ( 0.086 )\n",
      "test f1: 0.6082  ( 0.0911 )\n",
      "Basic average compressions:  26.000656250000002\n",
      "test accuracy: 0.61  ( 0.0644 )\n",
      "test f1: 0.6104  ( 0.0643 )\n",
      "Stacked average compressions:  26.000653645833335\n",
      "Threshold:  28\n",
      "test accuracy: 0.595  ( 0.064 )\n",
      "test f1: 0.5733  ( 0.0799 )\n",
      "Basic average compressions:  28.000935872395832\n",
      "test accuracy: 0.545  ( 0.1042 )\n",
      "test f1: 0.5334  ( 0.1126 )\n",
      "Stacked average compressions:  28.0009384765625\n",
      "Threshold:  30\n",
      "test accuracy: 0.54  ( 0.0663 )\n",
      "test f1: 0.5418  ( 0.0666 )\n",
      "Basic average compressions:  30.002237955729168\n",
      "test accuracy: 0.615  ( 0.0374 )\n",
      "test f1: 0.6142  ( 0.0391 )\n",
      "Stacked average compressions:  30.002237955729168\n",
      "Threshold:  32\n",
      "test accuracy: 0.625  ( 0.05 )\n",
      "test f1: 0.6121  ( 0.051 )\n",
      "Basic average compressions:  32.002522786458336\n",
      "test accuracy: 0.625  ( 0.0935 )\n",
      "test f1: 0.624  ( 0.0936 )\n",
      "Stacked average compressions:  32.002522786458336\n",
      "Threshold:  34\n",
      "test accuracy: 0.615  ( 0.097 )\n",
      "test f1: 0.5794  ( 0.1126 )\n",
      "Basic average compressions:  33.998738606770836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cidalab/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/cidalab/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/cidalab/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.56  ( 0.0957 )\n",
      "test f1: 0.5214  ( 0.1459 )\n",
      "Stacked average compressions:  33.9987412109375\n",
      "Threshold:  36\n",
      "test accuracy: 0.555  ( 0.0781 )\n",
      "test f1: 0.5162  ( 0.0858 )\n",
      "Basic average compressions:  36.000040690104164\n",
      "test accuracy: 0.61  ( 0.0624 )\n",
      "test f1: 0.6044  ( 0.0611 )\n",
      "Stacked average compressions:  36.000040690104164\n",
      "Threshold:  38\n",
      "test accuracy: 0.58  ( 0.0886 )\n",
      "test f1: 0.5782  ( 0.0859 )\n",
      "Basic average compressions:  38.000325520833336\n",
      "test accuracy: 0.64  ( 0.0604 )\n",
      "test f1: 0.6365  ( 0.0648 )\n",
      "Stacked average compressions:  38.000325520833336\n",
      "Threshold:  40\n",
      "test accuracy: 0.555  ( 0.09 )\n",
      "test f1: 0.5458  ( 0.097 )\n",
      "Basic average compressions:  40.00061295572917\n",
      "test accuracy: 0.625  ( 0.1025 )\n",
      "test f1: 0.619  ( 0.1066 )\n",
      "Stacked average compressions:  40.0006103515625\n",
      "Threshold:  42\n",
      "test accuracy: 0.61  ( 0.049 )\n",
      "test f1: 0.5769  ( 0.0734 )\n",
      "Basic average compressions:  42.001912434895836\n",
      "test accuracy: 0.61  ( 0.1179 )\n",
      "test f1: 0.5797  ( 0.1593 )\n",
      "Stacked average compressions:  42.001912434895836\n",
      "Threshold:  44\n",
      "test accuracy: 0.54  ( 0.0982 )\n",
      "test f1: 0.5055  ( 0.0968 )\n",
      "Basic average compressions:  44.00219986979167\n",
      "test accuracy: 0.63  ( 0.0696 )\n",
      "test f1: 0.626  ( 0.072 )\n",
      "Stacked average compressions:  44.002197265625\n",
      "Threshold:  46\n",
      "test accuracy: 0.575  ( 0.0822 )\n",
      "test f1: 0.5152  ( 0.1031 )\n",
      "Basic average compressions:  45.9984130859375\n",
      "test accuracy: 0.625  ( 0.1084 )\n",
      "test f1: 0.6258  ( 0.1077 )\n",
      "Stacked average compressions:  45.9984130859375\n",
      "Threshold:  48\n",
      "test accuracy: 0.615  ( 0.0903 )\n",
      "test f1: 0.5836  ( 0.1209 )\n",
      "Basic average compressions:  47.999715169270836\n",
      "test accuracy: 0.645  ( 0.0678 )\n",
      "test f1: 0.645  ( 0.0697 )\n",
      "Stacked average compressions:  47.999715169270836\n",
      "Threshold:  50\n",
      "test accuracy: 0.56  ( 0.1079 )\n",
      "test f1: 0.5333  ( 0.1405 )\n",
      "Basic average compressions:  50.0\n",
      "test accuracy: 0.64  ( 0.0644 )\n",
      "test f1: 0.6352  ( 0.0655 )\n",
      "Stacked average compressions:  50.0\n",
      "Threshold:  52\n",
      "test accuracy: 0.61  ( 0.0436 )\n",
      "test f1: 0.5871  ( 0.0429 )\n",
      "Basic average compressions:  52.000284830729164\n",
      "test accuracy: 0.6  ( 0.0822 )\n",
      "test f1: 0.6001  ( 0.0824 )\n",
      "Stacked average compressions:  52.000284830729164\n",
      "Threshold:  54\n",
      "test accuracy: 0.57  ( 0.0857 )\n",
      "test f1: 0.5489  ( 0.0928 )\n",
      "Basic average compressions:  54.0015869140625\n",
      "test accuracy: 0.595  ( 0.0797 )\n",
      "test f1: 0.5954  ( 0.0796 )\n",
      "Stacked average compressions:  54.0015869140625\n",
      "Threshold:  56\n",
      "test accuracy: 0.545  ( 0.1461 )\n",
      "test f1: 0.5291  ( 0.1576 )\n",
      "Basic average compressions:  55.997802734375\n",
      "test accuracy: 0.645  ( 0.0731 )\n",
      "test f1: 0.6389  ( 0.0796 )\n",
      "Stacked average compressions:  55.99780533854167\n",
      "Threshold:  58\n",
      "test accuracy: 0.59  ( 0.0718 )\n",
      "test f1: 0.5518  ( 0.0947 )\n",
      "Basic average compressions:  57.998087565104164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cidalab/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/cidalab/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/cidalab/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.58  ( 0.1111 )\n",
      "test f1: 0.5413  ( 0.1658 )\n",
      "Stacked average compressions:  57.998087565104164\n",
      "Threshold:  60\n",
      "test accuracy: 0.485  ( 0.0374 )\n",
      "test f1: 0.4483  ( 0.0324 )\n",
      "Basic average compressions:  59.9993896484375\n",
      "test accuracy: 0.63  ( 0.0992 )\n",
      "test f1: 0.6316  ( 0.0992 )\n",
      "Stacked average compressions:  59.9993896484375\n",
      "Threshold:  62\n",
      "test accuracy: 0.615  ( 0.0515 )\n",
      "test f1: 0.5801  ( 0.0703 )\n",
      "Basic average compressions:  61.999674479166664\n",
      "test accuracy: 0.625  ( 0.0975 )\n",
      "test f1: 0.6224  ( 0.0991 )\n",
      "Stacked average compressions:  61.999674479166664\n",
      "Threshold:  64\n",
      "test accuracy: 0.57  ( 0.0914 )\n",
      "test f1: 0.5489  ( 0.1125 )\n",
      "Basic average compressions:  63.999959309895836\n",
      "test accuracy: 0.655  ( 0.0992 )\n",
      "test f1: 0.6547  ( 0.1007 )\n",
      "Stacked average compressions:  63.999959309895836\n",
      "Threshold:  66\n",
      "test accuracy: 0.585  ( 0.1136 )\n",
      "test f1: 0.5772  ( 0.1064 )\n",
      "Basic average compressions:  66.00126139322917\n",
      "test accuracy: 0.635  ( 0.0718 )\n",
      "test f1: 0.6357  ( 0.0729 )\n",
      "Stacked average compressions:  66.00126139322917\n",
      "Threshold:  68\n",
      "test accuracy: 0.52  ( 0.0678 )\n",
      "test f1: 0.4677  ( 0.0913 )\n",
      "Basic average compressions:  67.99747721354167\n",
      "test accuracy: 0.625  ( 0.0758 )\n",
      "test f1: 0.6205  ( 0.076 )\n",
      "Stacked average compressions:  67.99747721354167\n",
      "Threshold:  70\n",
      "test accuracy: 0.655  ( 0.0579 )\n",
      "test f1: 0.633  ( 0.0827 )\n",
      "Basic average compressions:  69.99776204427083\n",
      "test accuracy: 0.63  ( 0.0714 )\n",
      "test f1: 0.6276  ( 0.0733 )\n",
      "Stacked average compressions:  69.99776204427083\n",
      "Threshold:  72\n",
      "test accuracy: 0.575  ( 0.1072 )\n",
      "test f1: 0.5705  ( 0.1114 )\n",
      "Basic average compressions:  71.99906412760417\n",
      "test accuracy: 0.63  ( 0.0992 )\n",
      "test f1: 0.6064  ( 0.1178 )\n",
      "Stacked average compressions:  71.99906412760417\n",
      "Threshold:  74\n",
      "test accuracy: 0.565  ( 0.0624 )\n",
      "test f1: 0.5173  ( 0.0883 )\n",
      "Basic average compressions:  73.99934895833333\n",
      "test accuracy: 0.645  ( 0.0967 )\n",
      "test f1: 0.6459  ( 0.0951 )\n",
      "Stacked average compressions:  73.99934895833333\n",
      "Threshold:  76\n",
      "test accuracy: 0.68  ( 0.062 )\n",
      "test f1: 0.6793  ( 0.064 )\n",
      "Basic average compressions:  75.9996337890625\n",
      "test accuracy: 0.635  ( 0.0663 )\n",
      "test f1: 0.6323  ( 0.0694 )\n",
      "Stacked average compressions:  75.9996337890625\n",
      "Threshold:  78\n",
      "test accuracy: 0.66  ( 0.0982 )\n",
      "test f1: 0.6557  ( 0.1 )\n",
      "Basic average compressions:  77.99686686197917\n",
      "test accuracy: 0.615  ( 0.0718 )\n",
      "test f1: 0.6121  ( 0.0707 )\n",
      "Stacked average compressions:  77.99686686197917\n"
     ]
    }
   ],
   "source": [
    "print(dbName)\n",
    "dbName = \"han/\"+dbName +\"/\"\n",
    "basic_f1s = []\n",
    "basic_compressions = []\n",
    "basic_stds = []\n",
    "\n",
    "stacked_f1s = []\n",
    "stacked_compressions = []\n",
    "stacked_stds = []\n",
    "\n",
    "for threshold in range(2, 80,2):\n",
    "    print(\"Threshold: \",threshold)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #training basic AE\n",
    "    model_compressions = []\n",
    "\n",
    "    BLL_Error, BFT_Error, basic_train_accuracy, basic_test_accuracy, basic_latent, basic_f1, basic_std = ae_training('basic',data_df, num_of_epoch, k_folds, threshold)\n",
    "\n",
    "\n",
    "    print('Basic average compressions: ', np.mean(model_compressions))\n",
    "    basic_f1s.append(basic_f1)\n",
    "    basic_compressions.append(np.mean(model_compressions))\n",
    "    basic_stds.append(basic_std)\n",
    "\n",
    "\n",
    "\n",
    "    #training stacked AE\n",
    "    model_compressions = []\n",
    "\n",
    "    SLL_Error, SFT_Error, stacked_train_accuracy, stacked_test_accuracy, stacked_latent, stacked_f1, stacked_std = ae_training('stacked',data_df, num_of_epoch, k_folds, threshold)\n",
    "\n",
    "    print('Stacked average compressions: ', np.mean(model_compressions))\n",
    "    stacked_f1s.append(stacked_f1)\n",
    "    stacked_compressions.append(np.mean(model_compressions))\n",
    "    stacked_stds.append(stacked_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "784c6919-7c17-4c2e-81d2-229ad2ce8990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(dbName+'compressions.pkl', 'wb') as f:\n",
    "    pickle.dump(basic_f1s, f)\n",
    "    pickle.dump(basic_compressions, f)\n",
    "    pickle.dump(basic_stds, f)\n",
    "    pickle.dump(stacked_f1s, f)\n",
    "    pickle.dump(stacked_compressions, f)\n",
    "    pickle.dump(stacked_stds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b3dcc1-bf50-4234-bb94-6954b2b66903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
